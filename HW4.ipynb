{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ECI 249 HW \\#4\n",
    "\n",
    "Kenneth Larrieu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Develop a stochastic dynamic program to find the optimal release policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class SDP:\n",
    "    def __init__(self):\n",
    "        self.stages = list(range(1, 21))\n",
    "        self.sts = list(range(0, 80, 10))\n",
    "        self.itm1s = list(range(10, 60, 10))\n",
    "        self.rts = list(range(0, 60, 10))\n",
    "        self.res_cap = 70\n",
    "        self.r_target = 30\n",
    "        self.trans_matrix = [[0.8, 0.1, 0.1, 0, 0],\n",
    "                             [0.3, 0.3, 0.3, 0.1, 0],\n",
    "                             [0.1, 0.3, 0.3, 0.2, 0.1],\n",
    "                             [0.1, 0.3, 0.3, 0.2, 0.1],\n",
    "                             [0.1, 0.2, 0.3, 0.3, 0.1]]\n",
    "        self.decision_dict = {}\n",
    "        self.obj_fn_dict = {}\n",
    "\n",
    "    def pen(self, rt, st, it):\n",
    "        penalty = 0\n",
    "        # if we are able to release the release decision amount\n",
    "        if rt <= st + it:\n",
    "            # and release decision is less than target amount\n",
    "            if rt < self.r_target:\n",
    "                penalty += 10 * self.r_target * (np.exp(2*(self.r_target-rt)/self.r_target) - 1)\n",
    "        # if we cannot release the release decision amount\n",
    "        else:\n",
    "            penalty += 5000\n",
    "            # in this case the actual amount released is st + it\n",
    "            # if the actual amount released is less than target\n",
    "            if st + it < self.r_target:\n",
    "                penalty += 10 * self.r_target * (np.exp(2*(self.r_target-(st + it))/self.r_target) - 1)\n",
    "\n",
    "        return penalty\n",
    "\n",
    "    def prob(self, it, itm1):\n",
    "        # Markovian probability of I_t given I_{t-1}\n",
    "        i1 = self.itm1s.index(itm1)\n",
    "        i2 = self.itm1s.index(it)\n",
    "        return self.trans_matrix[i1][i2]\n",
    "\n",
    "    def obj_fn(self, stage, rt, st, itm1):\n",
    "        # args: stage, state variables, and release decision\n",
    "        # iterate over possible I_t values and get corresponding penalty\n",
    "        # weight by probability of that I_t and sum\n",
    "        ev_penalty = 0\n",
    "        for it in self.itm1s:\n",
    "            # the penalty today for the given it\n",
    "            penalty_today = self.pen(rt, st, it)\n",
    "            # the accumulated penalty for the given it\n",
    "            if stage < self.stages[-1]:\n",
    "                i1 = self.sts.index(np.clip(st-rt+it, 0, self.res_cap))\n",
    "                i2 = self.itm1s.index(it)\n",
    "                accum_penalty = self.obj_fn_dict[stage+1][i1][i2]\n",
    "            else:\n",
    "                accum_penalty = 0\n",
    "\n",
    "            p = self.prob(it, itm1)\n",
    "\n",
    "            ev_penalty += (penalty_today + accum_penalty) * p\n",
    "\n",
    "        return ev_penalty\n",
    "\n",
    "    def get_best_decision(self, stage):\n",
    "        \"\"\"\n",
    "        At given stage, determine best decision at each state\n",
    "        Save decision (release value) and objective function value to corresponding dicts\n",
    "        \"\"\"\n",
    "        dec_array = []\n",
    "        obj_fn_array = []\n",
    "        # iterate over storage state var\n",
    "        for st in self.sts:\n",
    "            dec_row = []\n",
    "            obj_fn_val_row = []\n",
    "            # iterate over previous day inflow state var\n",
    "            for itm1 in self.itm1s:\n",
    "                # iterate over release decision var\n",
    "                best_rt = None\n",
    "                best_obj_fn_val = np.inf\n",
    "                for rt in self.rts:\n",
    "                    # evaluate objective function for each decision\n",
    "                    obj_fn_val = self.obj_fn(stage, rt, st, itm1)\n",
    "                    if obj_fn_val < best_obj_fn_val:\n",
    "                        best_rt = rt\n",
    "                        best_obj_fn_val = obj_fn_val\n",
    "                # keep best decision and corresponding obj fn value\n",
    "                dec_row.append(best_rt)\n",
    "                obj_fn_val_row.append(best_obj_fn_val)\n",
    "\n",
    "            dec_array.append(dec_row)\n",
    "            obj_fn_array.append(obj_fn_val_row)\n",
    "\n",
    "        self.decision_dict[stage] = dec_array\n",
    "        self.obj_fn_dict[stage] = obj_fn_array\n",
    "\n",
    "    def run_sdp(self):\n",
    "        for stage in self.stages[::-1]:\n",
    "            self.get_best_decision(stage)\n",
    "\n",
    "\n",
    "sdp = SDP()\n",
    "sdp.run_sdp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Produce a table indicating the optimal release policy as a function of current storage and previous inflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(sdp.decision_dict[1], index=sdp.sts, columns = sdp.itm1s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](SDP_policyy.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Very briefly identify some of the benefits and problems of solving such a problem by SDP.\n",
    "\n",
    " - Advantages: DP is vastly superior to brute force enumeration for identifying the optimal policy because it restricts the parameter space based on the optimal policy for future stages. SDP is also a good strategy because it accounts for uncertainties at play e.g. the future inflows to the reservoir.\n",
    " \n",
    " - Disadvantages: SDP uses discrete parameter values and stages. In addition, these discretized steps must be fairly coarse in order for SDP to be computationally tractable. As the number of decision/state variables increases, the problem becomes vastly more computationally expensive as well. Therefore, SDP can only practically be applied to vastly simplified versions of the system being modeled."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
